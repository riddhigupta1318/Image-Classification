{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install pycocotools\n!pip3 install pytorch_pretrained_bert","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:06:51.135349Z","iopub.execute_input":"2022-05-05T02:06:51.136188Z","iopub.status.idle":"2022-05-05T02:07:38.766497Z","shell.execute_reply.started":"2022-05-05T02:06:51.136063Z","shell.execute_reply":"2022-05-05T02:07:38.765617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nfrom collections import Counter\nimport nltk\nfrom PIL import Image\nimport imageio\nfrom pycocotools.coco import COCO\n\nclass Vocabulary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)\n\ndef build_vocab(json, threshold):\n    coco = COCO(json)\n    counter = Counter()\n    ids = coco.anns.keys()\n    for i, id in enumerate(ids):\n        caption = str(coco.anns[id]['caption'])\n        tokens = nltk.tokenize.word_tokenize(caption.lower())\n        counter.update(tokens)\n\n    # ommit non-frequent words\n    words = [word for word, cnt in counter.items() if cnt >= threshold]\n\n    vocab = Vocabulary()\n    vocab.add_word('<pad>') # 0\n    vocab.add_word('<start>') # 1\n    vocab.add_word('<end>') # 2\n    vocab.add_word('<unk>') # 3\n\n    for i, word in enumerate(words):\n        vocab.add_word(word)\n    return vocab\n\ndef resize_image(image):\n    width, height = image.size\n    if width > height:\n        left = (width - height) / 2\n        right = width - left\n        top = 0\n        bottom = height\n    else:\n        top = (height - width) / 2\n        bottom = height - top\n        left = 0\n        right = width\n    image = image.crop((left, top, right, bottom))\n    image = image.resize([224, 224], Image.ANTIALIAS)\n    return image\n\ndef main(caption_path,vocab_path,threshold):\n    vocab = build_vocab(json=caption_path,threshold=threshold)\n    with open(vocab_path, 'wb') as f:\n        pickle.dump(vocab, f)\n\n    # print(\"resizing images...\")\n    splits = ['val','train']\n\n#     for split in splits:\n#         folder = './data/%s2014' %split\n#         resized_folder = './data/%s2014_resized/' %split\n#         if not os.path.exists(resized_folder):\n#             os.makedirs(resized_folder)\n#         image_files = os.listdir(folder)\n#         num_images = len(image_files)\n#         for i, image_file in enumerate(image_files):\n#             with open(os.path.join(folder, image_file), 'r+b') as f:\n#                 with Image.open(f) as image:\n#                     image = resize_image(image)\n#                     image.save(os.path.join(resized_folder, image_file), image.format)\n\n    print(\"done resizing images...\")\n\ncaption_path = '../input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\nvocab_path = './vocab.pkl'\nthreshold = 5\n\nmain(caption_path,vocab_path,threshold)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:07:38.769629Z","iopub.execute_input":"2022-05-05T02:07:38.769928Z","iopub.status.idle":"2022-05-05T02:09:12.161486Z","shell.execute_reply.started":"2022-05-05T02:07:38.769881Z","shell.execute_reply":"2022-05-05T02:09:12.160605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coding: utf-8\n\nimport pickle\n# from processData import Vocabulary\nimport numpy as np\nimport json\nfrom scipy import misc\n# import bcolz\n\nwords = []\nidx = 0\nword2idx = {}\n# vectors = bcolz.carray(np.zeros(1), rootdir='./6B.300.dat', mode='w')\nvectors = []\n\nwith open('../input/glove6b/glove.6B.300d.txt', 'rb') as f:\n    for l in f:\n        line = l.decode().split()\n        word = line[0]\n        words.append(word)\n        word2idx[word] = idx\n        idx += 1\n        vect = np.array(line[1:]).astype(float)\n        vectors.append(vect)  \n# vectors = bcolz.carray(vectors[1:].reshape((4*-00000, 300)), rootdir='/6B.300.dat', mode='w')\n# vectors.flush()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:09:12.162946Z","iopub.execute_input":"2022-05-05T02:09:12.163208Z","iopub.status.idle":"2022-05-05T02:11:09.517778Z","shell.execute_reply.started":"2022-05-05T02:09:12.163171Z","shell.execute_reply":"2022-05-05T02:11:09.516981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(words, open('./6B.300_words.pkl', 'wb'),protocol=2)\npickle.dump(word2idx, open('./6B.300_idx.pkl', 'wb'), protocol=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:09.52295Z","iopub.execute_input":"2022-05-05T02:11:09.523538Z","iopub.status.idle":"2022-05-05T02:11:09.792597Z","shell.execute_reply.started":"2022-05-05T02:11:09.523497Z","shell.execute_reply":"2022-05-05T02:11:09.791721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./vocab.pkl', 'rb') as f:\n    vocab = pickle.load(f)\n\nprint('Loading vocab...')\n\n# vectors = bcolz.open('content/6B.300.dat')[:]\nvectors = np.asarray(vectors)\nwords = pickle.load(open('./6B.300_words.pkl', 'rb'))\nword2idx = pickle.load(open('./6B.300_idx.pkl', 'rb'))\n\nprint('glove is loaded...')\n\nglove = {w: vectors[word2idx[w]] for w in words}\nmatrix_len = len(vocab)\nweights_matrix = np.zeros((matrix_len, 300))\nwords_found = 0\n\nfor i, word in enumerate(vocab.idx2word):\n    try: \n        weights_matrix[i] = glove[word]\n        words_found += 1\n    except KeyError:\n        weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))\n\npickle.dump(weights_matrix, open('./glove_words.pkl', 'wb'), protocol=2)\n\nprint('weights_matrix is created')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:09.794044Z","iopub.execute_input":"2022-05-05T02:11:09.794369Z","iopub.status.idle":"2022-05-05T02:11:11.27286Z","shell.execute_reply.started":"2022-05-05T02:11:09.794327Z","shell.execute_reply":"2022-05-05T02:11:11.271985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nfrom torchvision import transforms\n\nclass DataLoader(data.Dataset):\n    def __init__(self, root, json, vocab, transform=None):\n\n        self.root = root\n        self.coco = COCO(json)\n        self.ids = list(self.coco.anns.keys())\n        self.vocab = vocab\n        self.transform = transform\n\n    def __getitem__(self, index):\n        coco = self.coco\n        vocab = self.vocab\n        ann_id = self.ids[index]\n        caption = coco.anns[ann_id]['caption']\n        img_id = coco.anns[ann_id]['image_id']\n        path = coco.loadImgs(img_id)[0]['file_name']\n\n        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)\n\n        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n        caption = []\n        caption.append(vocab('<start>'))\n        caption.extend([vocab(token) for token in tokens])\n        caption.append(vocab('<end>'))\n        target = torch.Tensor(caption)\n        return image, target\n\n    def __len__(self):\n        return len(self.ids)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:11.274418Z","iopub.execute_input":"2022-05-05T02:11:11.274687Z","iopub.status.idle":"2022-05-05T02:11:12.970565Z","shell.execute_reply.started":"2022-05-05T02:11:11.274651Z","shell.execute_reply":"2022-05-05T02:11:12.969694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef collate_fn(data):\n    data.sort(key=lambda  x: len(x[1]), reverse=True)\n    images, captions = zip(*data)\n\n    images = torch.stack(images, 0)\n\n    lengths = [len(cap) for cap in captions]\n    targets = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = cap[:end]\n    return images, targets, lengths\n\ndef get_loader(method, vocab, batch_size):\n\n    # train/validation paths\n    if method == 'train':\n        root = '../input/coco-2017-dataset/coco2017/train2017'\n        json = '../input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\n    elif method =='val':\n        root = '../input/coco-2017-dataset/coco2017/val2017'\n        json = '../input/coco-2017-dataset/coco2017/annotations/captions_val2017.json'\n\n    # resnet transformation/normalization\n    transform = transforms.Compose([\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                            (0.229, 0.224, 0.225))])\n\n    coco = DataLoader(root=root, json=json, vocab=vocab, transform=transform)\n\n    data_loader = torch.utils.data.DataLoader(dataset=coco,\n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=1,\n                                              collate_fn=collate_fn)\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:12.972139Z","iopub.execute_input":"2022-05-05T02:11:12.972384Z","iopub.status.idle":"2022-05-05T02:11:12.984975Z","shell.execute_reply.started":"2022-05-05T02:11:12.972347Z","shell.execute_reply":"2022-05-05T02:11:12.982034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport torch.nn as nn\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence\n# from data_loader import get_loader\n# DataLoader = DataLoader()\nfrom nltk.translate.bleu_score import corpus_bleu\n# from processData import Vocabulary\nfrom tqdm.notebook import tqdm\n# from tqdm import tqdm_notebook as tqdm\nimport torchvision.models as models\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\nimport json\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport skimage.transform\nimport argparse\nfrom scipy import misc\nfrom PIL import Image\nimport matplotlib.image as mpimg\nfrom IPython import display\nfrom torchtext.vocab import Vectors, GloVe\nfrom scipy import misc\nimport torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:12.986241Z","iopub.execute_input":"2022-05-05T02:11:12.986584Z","iopub.status.idle":"2022-05-05T02:11:13.460513Z","shell.execute_reply.started":"2022-05-05T02:11:12.986545Z","shell.execute_reply":"2022-05-05T02:11:13.45977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_vectors = pickle.load(open('./glove_words.pkl', 'rb'))\nglove_vectors = torch.tensor(glove_vectors)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:13.4617Z","iopub.execute_input":"2022-05-05T02:11:13.461976Z","iopub.status.idle":"2022-05-05T02:11:13.665409Z","shell.execute_reply.started":"2022-05-05T02:11:13.46194Z","shell.execute_reply":"2022-05-05T02:11:13.664662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Load pre-trained model (weights)\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:13.668303Z","iopub.execute_input":"2022-05-05T02:11:13.668645Z","iopub.status.idle":"2022-05-05T02:11:33.44967Z","shell.execute_reply.started":"2022-05-05T02:11:13.668605Z","shell.execute_reply":"2022-05-05T02:11:33.448979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model","metadata":{}},{"cell_type":"code","source":"#####################################\n# Encoder RESNET CNN - pretrained\n#####################################\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        resnet = models.resnet101(pretrained=True)\n        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n\n    def forward(self, images):\n        out = self.adaptive_pool(self.resnet(images))\n        # batch_size, img size, imgs size, 2048\n        out = out.permute(0, 2, 3, 1)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:33.450793Z","iopub.execute_input":"2022-05-05T02:11:33.452074Z","iopub.status.idle":"2022-05-05T02:11:33.459099Z","shell.execute_reply.started":"2022-05-05T02:11:33.452033Z","shell.execute_reply":"2022-05-05T02:11:33.458429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################\n# Attention Decoder\n####################\nclass Decoder(nn.Module):\n\n    def __init__(self, vocab_size, use_glove, use_bert):\n        super(Decoder, self).__init__()\n        self.encoder_dim = 2048\n        self.attention_dim = 512\n        self.use_bert = use_bert\n        if use_glove:\n            self.embed_dim = 300\n        elif use_bert:\n            self.embed_dim = 768\n        else:\n            self.embed_dim = 512\n            \n        self.decoder_dim = 512\n        self.vocab_size = vocab_size\n        self.dropout = 0.5\n        \n        # soft attention\n        self.enc_att = nn.Linear(2048, 512)\n        self.dec_att = nn.Linear(512, 512)\n        self.att = nn.Linear(512, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n        # decoder layers\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(self.embed_dim + self.encoder_dim, self.decoder_dim, bias=True)\n        self.h_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n        self.c_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n        self.f_beta = nn.Linear(self.decoder_dim, self.encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(self.decoder_dim, self.vocab_size)\n\n        # init variables\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n        \n        if not use_bert:\n            self.embedding = nn.Embedding(vocab_size, self.embed_dim)\n            self.embedding.weight.data.uniform_(-0.1, 0.1)\n\n            # load Glove embeddings\n            if use_glove:\n                self.embedding.weight = nn.Parameter(glove_vectors)\n\n            # always fine-tune embeddings (even with GloVe)\n            for p in self.embedding.parameters():\n                p.requires_grad = True\n            \n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):    \n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        dec_len = [x-1 for x in caption_lengths]\n        max_dec_len = max(dec_len)\n\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        if not self.use_bert:\n            embeddings = self.embedding(encoded_captions)\n        elif self.use_bert:\n            embeddings = []\n            for cap_idx in  encoded_captions:\n                \n                # padd caption to correct size\n                while len(cap_idx) < max_dec_len:\n                    cap_idx.append(PAD)\n                    \n                cap = ' '.join([vocab.idx2word[word_idx.item()] for word_idx in cap_idx])\n                cap = u'[CLS] '+cap\n                \n                tokenized_cap = tokenizer.tokenize(cap)                \n                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n                tokens_tensor = torch.tensor([indexed_tokens])\n\n                with torch.no_grad():\n                    encoded_layers, _ = model(tokens_tensor)\n\n                bert_embedding = encoded_layers[11].squeeze(0)\n                \n                split_cap = cap.split()\n                tokens_embedding = []\n                j = 0\n\n                for full_token in split_cap:\n                    curr_token = ''\n                    x = 0\n                    for i,_ in enumerate(tokenized_cap[1:]): # disregard CLS\n                        token = tokenized_cap[i+j]\n                        piece_embedding = bert_embedding[i+j]\n                        \n                        # full token\n                        if token == full_token and curr_token == '' :\n                            tokens_embedding.append(piece_embedding)\n                            j += 1\n                            break\n                        else: # partial token\n                            x += 1\n                            \n                            if curr_token == '':\n                                tokens_embedding.append(piece_embedding)\n                                curr_token += token.replace('#', '')\n                            else:\n                                tokens_embedding[-1] = torch.add(tokens_embedding[-1], piece_embedding)\n                                curr_token += token.replace('#', '')\n                                \n                                if curr_token == full_token: # end of partial\n                                    j += x\n                                    break                            \n                \n                               \n                cap_embedding = torch.stack(tokens_embedding)\n\n                embeddings.append(cap_embedding)\n                \n            embeddings = torch.stack(embeddings)\n\n        # init hidden state\n        avg_enc_out = encoder_out.mean(dim=1)\n        h = self.h_lin(avg_enc_out)\n        c = self.c_lin(avg_enc_out)\n\n        predictions = torch.zeros(batch_size, max_dec_len, vocab_size)\n        alphas = torch.zeros(batch_size, max_dec_len, num_pixels)\n\n        for t in range(max(dec_len)):\n            batch_size_t = sum([l > t for l in dec_len ])\n            \n            # soft-attention\n            enc_att = self.enc_att(encoder_out[:batch_size_t])\n            dec_att = self.dec_att(h[:batch_size_t])\n            att = self.att(self.relu(enc_att + dec_att.unsqueeze(1))).squeeze(2)\n            alpha = self.softmax(att)\n            attention_weighted_encoding = (encoder_out[:batch_size_t] * alpha.unsqueeze(2)).sum(dim=1)\n        \n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            \n            batch_embeds = embeddings[:batch_size_t, t, :]  \n            cat_val = torch.cat([batch_embeds.double(), attention_weighted_encoding.double()], dim=1)\n            \n            h, c = self.decode_step(cat_val.float(),(h[:batch_size_t].float(), c[:batch_size_t].float()))\n            preds = self.fc(self.dropout(h))\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n            \n        # preds, sorted capts, dec lens, attention wieghts\n        return predictions, encoded_captions, dec_len, alphas","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:33.460115Z","iopub.execute_input":"2022-05-05T02:11:33.460322Z","iopub.status.idle":"2022-05-05T02:11:34.274332Z","shell.execute_reply.started":"2022-05-05T02:11:33.460295Z","shell.execute_reply":"2022-05-05T02:11:34.273358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Setup","metadata":{}},{"cell_type":"code","source":"# loss\nclass loss_obj(object):\n    def __init__(self):\n        self.avg = 0.\n        self.sum = 0.\n        self.count = 0.\n\n    def update(self, val, n=1):\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nPAD = 0\nSTART = 1\nEND = 2\nUNK = 3\n\ngrad_clip = 5.\nnum_epochs = 3\nbatch_size = 32\ndecoder_lr = 0.0004\n\n# Load vocabulary wrapper\nwith open('./vocab.pkl', 'rb') as f:\n    vocab = pickle.load(f)\n\n# load data\ntrain_loader = get_loader('train', vocab, batch_size)\nval_loader = get_loader('val', vocab, batch_size)\n\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:34.275771Z","iopub.execute_input":"2022-05-05T02:11:34.276261Z","iopub.status.idle":"2022-05-05T02:11:36.062265Z","shell.execute_reply.started":"2022-05-05T02:11:34.276218Z","shell.execute_reply":"2022-05-05T02:11:36.061463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vocab)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:36.063555Z","iopub.execute_input":"2022-05-05T02:11:36.063992Z","iopub.status.idle":"2022-05-05T02:11:36.069889Z","shell.execute_reply.started":"2022-05-05T02:11:36.063955Z","shell.execute_reply":"2022-05-05T02:11:36.068947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#############\n# New model\n#############\n\ndecoder = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True) # true = use glove\ndecoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr)\n\nencoder = Encoder()\n\n#############\n# Load model\n#############\n\n# encoder = Encoder()\n# encoder_checkpoint = torch.load('./encoder_4')\n# encoder.load_state_dict(encoder_checkpoint['model_state_dict'])\n# decoder = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True)\n# decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr)\n# decoder_checkpoint = torch.load('./decoder_4',map_location='cpu')\n# # print(decoder_checkpoint['optimizer_state_dict'])\n# decoder.load_state_dict(decoder_checkpoint['model_state_dict'])\n# decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:11:36.071199Z","iopub.execute_input":"2022-05-05T02:11:36.07152Z","iopub.status.idle":"2022-05-05T02:12:12.483814Z","shell.execute_reply.started":"2022-05-05T02:11:36.071483Z","shell.execute_reply":"2022-05-05T02:12:12.480827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train","metadata":{}},{"cell_type":"code","source":"\ndef train():\n    print(\"Started training...\")\n    for epoch in tqdm(range(num_epochs)):\n        decoder.train()\n        encoder.train()\n\n        losses = loss_obj()\n        num_batches = len(train_loader)\n        print('num batches', num_batches)\n\n        for i, (imgs, caps, caplens) in enumerate(tqdm(train_loader)):\n            if i > 100:\n                break\n            imgs = encoder(imgs)\n\n            scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n\n            targets = caps_sorted[:, 1:]\n            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n\n            loss = criterion(scores, targets)\n            loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            decoder_optimizer.zero_grad()\n            loss.backward()\n\n            # grad_clip decoder\n            for group in decoder_optimizer.param_groups:\n                for param in group['params']:\n                    if param.grad is not None:\n                        param.grad.data.clamp_(-grad_clip, grad_clip)\n\n            decoder_optimizer.step()\n\n            losses.update(loss.item(), sum(decode_lengths))\n            print('Batch '+str(i)+'/'+str(num_batches)+' loss:'+str(losses.avg))\n\n            # save model each 100 batches\n            if i%20==0 and i!=0:\n                print('Batch '+str(i)+'/'+str(num_batches)+' loss:'+str(losses.avg))\n                \n                 # adjust learning rate (create condition for this)\n                for param_group in decoder_optimizer.param_groups:\n                    param_group['lr'] = param_group['lr'] * 0.5\n\n                print('saving model...')\n\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': decoder.state_dict(),\n                    'optimizer_state_dict': decoder_optimizer.state_dict(),\n                    'loss': loss,\n                    }, './decoder_baseline')\n\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': encoder.state_dict(),\n                    'loss': loss,\n                    }, './encoder_baseline')\n\n                print('model saved')\n        \n        print('Epoch Loss:'+str(losses.avg))\n\n    print(\"Completed training...\")  \n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:12:12.490494Z","iopub.execute_input":"2022-05-05T02:12:12.490718Z","iopub.status.idle":"2022-05-05T02:12:12.513988Z","shell.execute_reply.started":"2022-05-05T02:12:12.490689Z","shell.execute_reply":"2022-05-05T02:12:12.512247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Validate","metadata":{}},{"cell_type":"code","source":"\n%matplotlib inline\n\ndef print_sample(hypotheses,references,imgs, alphas, k, show_att):\n    print('Baseline Model')\n    img_dim = 500 # 14*24\n    \n    hyp_sentence = []\n    for word_idx in hypotheses[k]:\n        hyp_sentence.append(vocab.idx2word[word_idx])\n    \n    ref_sentence = []\n    for word_idx in references[k]:\n        ref_sentence.append(vocab.idx2word[word_idx])\n        \n    img = imgs[0][k] \n#     misc.imsave('img.jpg', img)\n    imageio.imwrite('img.jpg', img)\n  \n    if show_att:\n        image = Image.open('img.jpg')\n        image = image.resize([img_dim, img_dim], Image.LANCZOS)\n        for t in range(len(hyp_sentence)):\n\n            plt.subplot(np.ceil(len(hyp_sentence) / 5.), 5, t + 1)\n\n            plt.text(0, 1, '%s' % (hyp_sentence[t]), color='black', backgroundcolor='white', fontsize=12)\n            plt.imshow(image,cmap='gray')\n            current_alpha = alphas[0][t, :].detach().numpy()\n            alpha = skimage.transform.resize(current_alpha, [img_dim, img_dim])\n            if t == 0:\n                plt.imshow(alpha, alpha=0,cmap='gray')\n            else:\n                plt.imshow(alpha, alpha=0.7,cmap='gray')\n            plt.axis('off')\n    else:\n        img = imageio.imread('img.jpg')\n        plt.imshow(img)\n        plt.axis('off')\n        plt.show()\n        \n    print('Hypotheses: '+\" \".join(hyp_sentence))\n    print('References: '+\" \".join(ref_sentence))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:12:12.515814Z","iopub.execute_input":"2022-05-05T02:12:12.516981Z","iopub.status.idle":"2022-05-05T02:12:12.722148Z","shell.execute_reply.started":"2022-05-05T02:12:12.516685Z","shell.execute_reply":"2022-05-05T02:12:12.71995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"references = [] \ntest_references = []\nhypotheses = [] \nall_imgs = []\nall_alphas = []\n\ndef validate():\n    print(\"Started validation...\")\n    decoder.eval()\n    encoder.eval()\n\n    losses = loss_obj()\n\n    num_batches = len(val_loader)\n    # Batches\n    for i, (imgs, caps, caplens) in enumerate(tqdm(val_loader)):\n        if i > 10:\n            break\n\n        imgs_jpg = imgs.numpy() \n        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n        \n        # Forward prop.\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n        targets = caps_sorted[:, 1:]\n\n        # Remove timesteps that we didn't decode at, or are pads\n        scores_packed = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n        targets_packed = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n\n        # Calculate loss\n        loss = criterion(scores_packed, targets_packed)\n        loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        losses.update(loss.item(), sum(decode_lengths))\n\n         # References\n        for j in range(targets.shape[0]):\n            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n            img_captions = list(map(lambda c: clean_cap,img_caps))\n            test_references.append(clean_cap)\n            references.append(img_captions)\n\n        # Hypotheses\n        _, preds = torch.max(scores, dim=2)\n        preds = preds.tolist()\n        temp_preds = list()\n        for j, p in enumerate(preds):\n            pred = p[:decode_lengths[j]]\n            pred = [w for w in pred if w not in [PAD, START, END]]\n            temp_preds.append(pred)  # remove pads, start, and end\n        preds = temp_preds\n        hypotheses.extend(preds)\n        \n        all_alphas.append(alphas)\n        all_imgs.append(imgs_jpg)\n\n    bleu = corpus_bleu(references, hypotheses)\n    bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n    bleu_2 = corpus_bleu(references, hypotheses, weights=(0, 1, 0, 0))\n    bleu_3 = corpus_bleu(references, hypotheses, weights=(0, 0, 1, 0))\n    bleu_4 = corpus_bleu(references, hypotheses, weights=(0, 0, 0, 1))\n\n    print(\"Validation loss: \"+str(losses.avg))\n    print(\"BLEU: \"+str(bleu))\n    print(\"BLEU-1: \"+str(bleu_1))\n    print(\"BLEU-2: \"+str(bleu_2))\n    print(\"BLEU-3: \"+str(bleu_3))\n    print(\"BLEU-4: \"+str(bleu_4))\n    print_sample(hypotheses, test_references, all_imgs, all_alphas,1,False)\n    print(\"Completed validation...\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:12:12.723977Z","iopub.execute_input":"2022-05-05T02:12:12.724408Z","iopub.status.idle":"2022-05-05T02:12:12.838357Z","shell.execute_reply.started":"2022-05-05T02:12:12.724371Z","shell.execute_reply":"2022-05-05T02:12:12.837489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\ndef compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, imgs, k):\n    img_dim = 500 # 14*24\n    \n    bert_hyp_sentence = []\n    for word_idx in bert_hypotheses[k]:\n        bert_hyp_sentence.append(vocab.idx2word[word_idx])\n        \n    glove_hyp_sentence = []\n    for word_idx in glove_hypotheses[k]:\n        glove_hyp_sentence.append(vocab.idx2word[word_idx])\n    \n    baseline_hyp_sentence = []\n    for word_idx in baseline_hypotheses[k]:\n        baseline_hyp_sentence.append(vocab.idx2word[word_idx])\n    \n    ref_sentence = []\n    for word_idx in references[k]:\n        ref_sentence.append(vocab.idx2word[word_idx])\n        \n    img = imgs[0][k] \n    imageio.imwrite('img.jpg', img)\n#     misc.imsave('img.jpg', img)\n  \n#     img = misc.imread('img.jpg')\n    img = imageio.imread('img.jpg')\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()\n        \n    print('References: '+\" \".join(ref_sentence))\n    print(' ')\n    print('Baseline  : '+\" \".join(baseline_hyp_sentence))\n    print('GloVe     : '+\" \".join(glove_hyp_sentence))\n    print('BERT      : '+\" \".join(bert_hyp_sentence))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:12:12.84435Z","iopub.execute_input":"2022-05-05T02:12:12.844586Z","iopub.status.idle":"2022-05-05T02:12:12.942692Z","shell.execute_reply.started":"2022-05-05T02:12:12.844558Z","shell.execute_reply":"2022-05-05T02:12:12.941784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"references = [] \nbert_hypotheses = []\nglove_hypotheses = [] \nbaseline_hypotheses = [] \nall_imgs = []\n\ndef compare_models():\n    \n    # load all pre-trained models\n    \n    encoder_glove = Encoder()\n    encoder_checkpoint = torch.load('./checkpoints/encoder_glove',map_location='cpu')\n    encoder_glove.load_state_dict(encoder_checkpoint['model_state_dict'])\n    decoder_glove = Decoder(vocab_size=len(vocab),use_glove=True, use_bert=False)\n    decoder_optimizer = torch.optim.Adam(params=decoder_glove.parameters(),lr=decoder_lr)\n    decoder_checkpoint = torch.load('./checkpoints/decoder_glove',map_location='cpu')\n    decoder_glove.load_state_dict(decoder_checkpoint['model_state_dict'])\n    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n\n    #########\n\n    encoder_bert = Encoder()\n    encoder_checkpoint = torch.load('./checkpoints/encoder_bert',map_location='cpu')\n    encoder_bert.load_state_dict(encoder_checkpoint['model_state_dict'])\n    decoder_bert = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True)\n    decoder_optimizer = torch.optim.Adam(params=decoder_bert.parameters(),lr=decoder_lr)\n    decoder_checkpoint = torch.load('./checkpoints/decoder_bert',map_location='cpu')\n    decoder_bert.load_state_dict(decoder_checkpoint['model_state_dict'])\n    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n\n    #########\n\n    encoder_baseline = Encoder()\n    encoder_checkpoint = torch.load('./checkpoints/encoder_baseline',map_location='cpu')\n    encoder_baseline.load_state_dict(encoder_checkpoint['model_state_dict'])\n    decoder_baseline = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=False)\n    decoder_optimizer = torch.optim.Adam(params=decoder_baseline.parameters(),lr=decoder_lr)\n    decoder_checkpoint = torch.load('./checkpoints/decoder_baseline',map_location='cpu')\n    decoder_baseline.load_state_dict(decoder_checkpoint['model_state_dict'])\n    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n\n    print(\"Started Comparison...\")\n    decoder_bert.eval()\n    encoder_bert.eval()\n    decoder_glove.eval()\n    decoder_baseline.eval()\n\n    # Batches\n    for i, (imgs, caps, caplens) in enumerate(tqdm(val_loader)):\n        if i > 0:\n            break\n\n        imgs_jpg = imgs.numpy() \n        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n        \n        # Forward prop.\n        imgs = encoder_bert(imgs)\n        scores_bert, caps_sorted_bert,decode_lengths_bert , _ = decoder_bert(imgs, caps, caplens)\n        targets = caps_sorted_bert[:, 1:]\n        scores_glove, caps_sorted_glove, decode_lengths_glove, _ = decoder_glove(imgs, caps, caplens)\n        scores_baseline, caps_sorted_baseline, decode_lengths_baseline, _ = decoder_baseline(imgs, caps, caplens)\n\n         # References\n        for j in range(targets.shape[0]):\n            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n            img_captions = list(map(lambda c: clean_cap,img_caps))\n            references.append(clean_cap)\n\n        # Hypotheses\n        _, preds_bert = torch.max(scores_bert, dim=2)\n        _, preds_glove = torch.max(scores_glove, dim=2)\n        _, preds_baseline = torch.max(scores_baseline, dim=2)\n        preds_bert = preds_bert.tolist()\n        preds_glove = preds_glove.tolist()\n        preds_baseline = preds_baseline.tolist()\n        \n        temp_preds_bert = list()\n        temp_preds_glove = list()\n        temp_preds_baseline = list()\n        \n        for j, p in enumerate(preds_bert):\n            pred = preds_bert[j][:decode_lengths_bert[j]]\n            pred = [w for w in pred if w not in [PAD, START, END]]\n            temp_preds_bert.append(pred)  # remove pads, start, and end\n            \n            pred = preds_glove[j][:decode_lengths_glove[j]]\n            pred = [w for w in pred if w not in [PAD, START, END]]\n            temp_preds_glove.append(pred)  # remove pads, start, and end\n            \n            pred = preds_baseline[j][:decode_lengths_baseline[j]]\n            pred = [w for w in pred if w not in [PAD, START, END]]\n            temp_preds_baseline.append(pred)  # remove pads, start, and end\n            \n        bert_hypotheses.extend(temp_preds_bert)\n        glove_hypotheses.extend(temp_preds_glove)\n        baseline_hypotheses.extend(temp_preds_baseline)\n        all_imgs.append(imgs_jpg)\n        \n        compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, all_imgs, 1)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:12:12.946991Z","iopub.execute_input":"2022-05-05T02:12:12.947213Z","iopub.status.idle":"2022-05-05T02:12:13.047736Z","shell.execute_reply.started":"2022-05-05T02:12:12.947187Z","shell.execute_reply":"2022-05-05T02:12:13.046927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:12:13.052313Z","iopub.execute_input":"2022-05-05T02:12:13.054425Z","iopub.status.idle":"2022-05-05T02:14:12.5854Z","shell.execute_reply.started":"2022-05-05T02:12:13.05438Z","shell.execute_reply":"2022-05-05T02:14:12.582295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_CUDA = torch.cuda.is_available()\nDEVICE=torch.device('cuda:0') # or set to 'cpu'\nprint(\"CUDA:\", USE_CUDA)\nprint(DEVICE)\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:14:12.588976Z","iopub.status.idle":"2022-05-05T02:14:12.589305Z","shell.execute_reply.started":"2022-05-05T02:14:12.58914Z","shell.execute_reply":"2022-05-05T02:14:12.589163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validate()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T02:14:12.593327Z","iopub.status.idle":"2022-05-05T02:14:12.593614Z","shell.execute_reply.started":"2022-05-05T02:14:12.593467Z","shell.execute_reply":"2022-05-05T02:14:12.593485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}